











  1%|          | 11/1516 [02:14<5:59:19, 14.33s/it]

















  1%|▏         | 21/1516 [11:25<5:45:43, 13.88s/it]
















  2%|▏         | 31/1516 [14:23<3:55:58,  9.53s/it]















  3%|▎         | 41/1516 [16:48<3:32:24,  8.64s/it]















  3%|▎         | 51/1516 [19:37<3:52:21,  9.52s/it]

















  4%|▍         | 61/1516 [28:07<8:31:51, 21.11s/it]












  4%|▍         | 66/1516 [31:48<11:38:55, 28.92s/it]
Traceback (most recent call last):
  File "/Users/taohuadao/Downloads/UU/semester3/R&D/OffensiveLanguage/run.py", line 288, in <module>
    dev_loader = Data.DataLoader(dev_set, batch_size=cfg['batch_size'], shuffle=True)
  File "/Users/taohuadao/Downloads/UU/semester3/R&D/OffensiveLanguage/run.py", line 161, in train
    dev_loss = eval(cur_step, model, dev_loader, lr_scheduler)
  File "/Users/taohuadao/opt/anaconda3/envs/daily/lib/python3.8/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/Users/taohuadao/opt/anaconda3/envs/daily/lib/python3.8/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
  File "/Users/taohuadao/opt/anaconda3/envs/daily/lib/python3.8/site-packages/wandb/wandb_torch.py", line 282, in <lambda>
    handle = var.register_hook(lambda grad: _callback(grad, log_track))
