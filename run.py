# -*- coding: utf-8 -*-
"""Glacian-English

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hnw4suDP5CiiACacou2vC10xbYsMfVDy
"""
import gc
import os
import numpy as np
import torch
import torch.nn as nn
import torch.utils.data as Data
from tqdm import tqdm
from sklearn.metrics import *
from transformers import *
from utils import *
import argparse
from datetime import timedelta
import wandb
from nltk.translate.bleu_score import corpus_bleu
from sklearn.metrics import f1_score
try:
    from apex import amp
    USE_AMP = False #True
except ImportError:
    USE_AMP = False
    print('Warning: Apex not installed.')


# transformers.logging.set_verbosity_error()

# save model
def save_model(cfg, logger, step, model, optimizer, lr_scheduler):
    model_to_save = model.module if hasattr(model, 'module') else model
    # save static dict
    checkpoint = {
        'model': model_to_save.state_dict(),
        'optimizer': optimizer.state_dict(),
        'lr_scheduler': lr_scheduler.state_dict()
    }
    torch.save(checkpoint, get_ckpt_path(cfg).format(step))
    logger.info('save train model.')

# seeds
def set_seeds(SEED):
    np.random.seed(SEED)
    torch.manual_seed(SEED)

class ParallelCorpus(Data.Dataset):
    def __init__(self, data_path, cfg):
        self.data = self.process_data(data_path)
        self.tokenizer = XLMRobertaTokenizer.from_pretrained(cfg['model_name'])
        self.body_maxlen = cfg['body_maxlen']

    def process_data(self, data_paths):
        data = []
        for data_path in data_paths:
            with open(get_abs_path(data_path)) as f:
                for line in f.readlines():
                    data.append([line.split('\t')[0], int(line.split('\t')[1].strip())])
            logger.info("Read {} pieces of samples".format(len(data)))
        return data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        source = self.data[index][0]
        target = self.data[index][1]

        batch = self.tokenizer(source,
                             padding='max_length',
                             truncation=True,
                             max_length=self.body_maxlen,
                             return_tensors='pt')
        input_ids = batch['input_ids'].squeeze(0)

        return input_ids, torch.tensor(target, dtype=torch.long)


class MBart(nn.Module):
    def __init__(self, freeze_bert=False, model_name='microsoft/Multilingual-MiniLM-L12-H384'):
        super(MBart, self).__init__()
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name, return_dict=True)
        if freeze_bert:
            for p in self.model.parameters():
                p.requires_grad = False

    def forward(self, input_ids, labels):
        outputs = self.model(input_ids=input_ids, labels=labels)
        return outputs.loss

    def generate(self, input_ids, labels, decoder_start_token):
        generated_tokens = self.bert.generate(input_ids, decoder_start_token_id=self.tokenizer.lang_code_to_id[
            decoder_start_token])
        generated_sentence = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
        ground_truth_sentence = self.tokenizer.batch_decode(labels, skip_special_tokens=True)
        return generated_sentence, ground_truth_sentence


def train(model, optimizer, train_loader, dev_loader, epochs=1):
    logger.info('----Training----')

    # wandb
    if cfg['local_rank'] == 0:
        wandb.login(key='f841d6899cafcfbed913f5082886a4fe4af52c31')
        wandb.init(
            project=cfg['task']
        )
        wandb.watch(model, log='all', log_freq=100)

    # apex
    if USE_AMP:
        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')

    # model
    if cfg['is_distributed']:
        model = torch.nn.parallel.DistributedDataParallel(
            model,
            device_ids=[cfg['local_rank']],
            output_device=cfg['local_rank']
        )

    local_batch_counts = cfg['epoch_num'] * int(len(train_loader))  # batch count in one GPU
    training_steps = int(np.ceil(local_batch_counts / cfg['accum_steps']))
    lr_scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=cfg['warmup_steps'],
        num_training_steps=training_steps
    )
    accum_steps = cfg['accum_steps']
    cur_step = 1
    for epoch in range(epochs):
        model.train()
        logger.info('Epoch {}'.format(epoch))
        for i, data in enumerate(tqdm(train_loader)):
            batch = tuple(t.to(device) for t in data)
            inputs = {"input_ids": batch[0], "labels": batch[1]}
            outputs = model(**inputs)
            loss = outputs[0]
            logger.info("step {}, training loss {}".format(i, loss.item()))
            loss = loss / accum_steps

            # apex
            if USE_AMP:
                with amp.scale_loss(loss, optimizer) as scaled_loss:
                    scaled_loss.backward()
            else:
                loss.backward()

            if (i + 1) % accum_steps == 0 or (i + 1) == int(len(train_loader)):
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

                if ((i - 1 and i % cfg['eval_step'] == 1) or (i + 1) == int(len(train_loader))) and cfg['local_rank'] == 0:
                    dev_loss = eval(cur_step, model, dev_loader, lr_scheduler)
                    metrics = dict()
                    metrics['train/loss'] = loss.item()
                    metrics['dev/loss'] = dev_loss
                    # metrics['bleu'] = dev_bleu
                    wandb.log(metrics)

            if cfg['is_distributed']: torch.distributed.barrier()
            cur_step += 1
        # epoch end
        if epoch == epochs - 1:
            save_model(cfg, logger, cur_step, model, optimizer, lr_scheduler)

    if cfg['local_rank'] == 0:
        wandb.finish()


def eval(cur_step, model, val_dataloader, lr_scheduler=None):
    logger.info('Eval starts for current step {}'.format(cur_step))
    device = torch.device("cuda", cfg['local_rank']) if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)
    model.eval()
    eval_loss, eval_steps, preds, out_label_ids = 0, 0, None, None
    with torch.no_grad():
        for idx, data in enumerate(tqdm(val_dataloader)):
            eval_steps += 1
            batch = tuple(t.to(device) for t in data)
            inputs = {"input_ids": batch[0], "labels": batch[1]}
            outputs = model(**inputs)
            loss, logits = outputs[:2]
            eval_loss += loss.item()
            if preds is None:
                preds = logits.detach().cpu().numpy()
                out_label_ids = inputs["labels"].detach().cpu().numpy()
            else:
                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
                out_label_ids = np.append(out_label_ids, inputs["labels"].detach().cpu().numpy(), axis=0)
    preds = np.argmax(preds, axis=1)
    val_f1 = f1_score(preds, out_label_ids)
    if lr_scheduler and val_f1 < cfg['best_f1']:
        cfg['best_f1'] = val_f1
        logger.info('Congrats!')
        ckpt_list.append(get_ckpt_path(cfg).format(cur_step))
        if len(ckpt_list) > cfg['max_ckpt']:
            os.remove(ckpt_list.pop(0))
        save_model(cfg, logger, cur_step, model, optimizer, lr_scheduler)

    model.train()
    logger.info('step {}, validation loss: {}, f1 score: {}'.format(cur_step, eval_loss / eval_steps, val_f1))
    return eval_loss / eval_steps

def bleu_score(ref_list, hyp_list):
    references = [[ref.split()] for ref in ref_list]
    hypotheses = [hpy.split() for hpy in hyp_list]
    return corpus_bleu(references, hypotheses)

def predict(model, pred_dataloader):
    device = torch.device("cuda", cfg['local_rank']) if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)
    model.eval()
    predicts, references = [], []
    with torch.no_grad():
        for idx, batch in enumerate(tqdm(pred_dataloader)):
            batch = tuple(t.to(device) for t in batch)
            generated_sentence, ground_truth_sentence = model.generate(batch[0], batch[1], 'en_XX')
            references.extend(ground_truth_sentence)
            predicts.extend(generated_sentence)
    bleu = bleu_score(references, predicts)
    logger.info("Corpus BLEU score: {}".format(bleu))
    return predicts, references



if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='News Headline Generation')
    parser.add_argument('-c', '--config', default='config.json', type=str, required=False,
                        help='config file path')
    parser.add_argument('-t', '--type', default='train', type=str, required=False, help='train / eval / predict')
    args, unknown = parser.parse_known_args()

    cfg = load_json(get_abs_path('config.json'))
    cfg['time_str'] = args.type + '_' + get_datetime()
    set_seeds(cfg['seeds'])

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    ckpt_list = []

    # gpu
    os.environ["CUDA_VISIBLE_DEVICES"] = cfg['gpu']
    cfg['is_distributed'] = len(cfg['gpu'].split(',')) > 1
    cfg['local_rank'] = 0

    # dpp
    if cfg['is_distributed'] and args.type == 'train':
        torch.distributed.init_process_group(backend="nccl", timeout=timedelta(hours=4))
        local_rank = torch.distributed.get_rank()
        torch.cuda.set_device(local_rank)
        cfg['local_rank'] = local_rank
        torch.cuda.synchronize()

    # logger
    logger = get_logger(cfg)
    logger.info(str(cfg))

    # model = MBart(freeze_bert=False, model_name=cfg['model_name'])
    model = BertForSequenceClassification.from_pretrained(cfg['model_name'], return_dict=True)
    optimizer = AdamW(model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])

    if os.path.exists(cfg['output_model']):
        logger.info('Loading model...')
        checkpoint = torch.load(cfg['output_model'], map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    else:
        model.to(device)

    if args.type == 'train':
        # train_data = process_data(cfg['train_src_path'], cfg['train_tgt_path'])
        # dev_data = process_data(cfg['dev_src_path'], cfg['dev_tgt_path'])

        print('Reading training data...')
        train_set = ParallelCorpus(cfg['train_data_path'], cfg)
        train_loader = Data.DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True)

        print('Reading development data...')
        dev_set = ParallelCorpus(cfg['dev_data_path'], cfg)
        dev_loader = Data.DataLoader(dev_set, batch_size=cfg['batch_size'], shuffle=True)
        train(model, optimizer, train_loader, dev_loader, epochs=cfg['epoch_num'])
    elif args.type == 'eval':
        # dev_data = process_data(cfg['dev_src_path'], cfg['dev_tgt_path'])
        print('Reading development data...')
        dev_set = ParallelCorpus(cfg['dev_data_path'], cfg)
        dev_loader = Data.DataLoader(dev_set, batch_size=cfg['batch_size'], shuffle=True)
        eval(0, model, dev_loader)
    else:
        # test_data = process_data(cfg['test_src_path'], cfg['test_tgt_path'])
        print('Reading development data...')
        test_set = ParallelCorpus(cfg['test_data_path'], cfg)
        pred_loader = Data.DataLoader(test_set, batch_size=cfg['batch_size'], shuffle=True)
        predict(model, pred_loader)
